{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d7cbaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page 0 of 40\n",
      "Getting page 1 of 40\n",
      "Getting page 2 of 40\n",
      "Getting page 3 of 40\n",
      "Getting page 4 of 40\n",
      "Getting page 5 of 40\n",
      "Getting page 6 of 40\n",
      "Getting page 7 of 40\n",
      "Getting page 8 of 40\n",
      "Getting page 9 of 40\n",
      "Getting page 10 of 40\n",
      "Getting page 11 of 40\n",
      "Getting page 12 of 40\n",
      "Getting page 13 of 40\n",
      "Getting page 14 of 40\n",
      "Getting page 15 of 40\n",
      "Getting page 16 of 40\n",
      "Getting page 17 of 40\n",
      "Getting page 18 of 40\n",
      "Getting page 19 of 40\n",
      "Getting page 20 of 40\n",
      "Getting page 21 of 40\n",
      "Getting page 22 of 40\n",
      "Getting page 23 of 40\n",
      "Getting page 24 of 40\n",
      "Getting page 25 of 40\n",
      "Getting page 26 of 40\n",
      "Getting page 27 of 40\n",
      "Getting page 28 of 40\n",
      "Getting page 29 of 40\n",
      "Getting page 30 of 40\n",
      "Getting page 31 of 40\n",
      "Getting page 32 of 40\n",
      "Getting page 33 of 40\n",
      "Getting page 34 of 40\n",
      "Getting page 35 of 40\n",
      "Getting page 36 of 40\n",
      "Getting page 37 of 40\n",
      "Getting page 38 of 40\n",
      "Getting page 39 of 40\n",
      "Getting page 40 of 40\n",
      "Getting page 41 of 40\n",
      "Getting page 42 of 40\n",
      "Getting page 43 of 40\n",
      "Getting page 44 of 40\n",
      "Getting page 45 of 40\n",
      "Getting page 46 of 40\n",
      "Getting page 47 of 40\n",
      "Getting page 48 of 40\n",
      "Getting page 49 of 40\n",
      "Getting page 50 of 40\n",
      "Getting page 51 of 40\n",
      "Getting page 52 of 40\n",
      "Getting page 53 of 40\n",
      "Getting page 54 of 40\n",
      "Getting page 55 of 40\n",
      "Getting page 56 of 40\n",
      "Getting page 57 of 40\n",
      "Getting page 58 of 40\n",
      "Getting page 59 of 40\n",
      "Getting page 60 of 40\n",
      "Getting page 61 of 40\n",
      "Getting page 62 of 40\n",
      "Getting page 63 of 40\n",
      "Getting page 64 of 40\n",
      "Getting page 65 of 40\n",
      "Getting page 66 of 40\n",
      "Getting page 67 of 40\n",
      "Getting page 68 of 40\n",
      "Getting page 69 of 40\n",
      "Getting page 70 of 40\n",
      "Getting page 71 of 40\n",
      "Data saved to titles_urls_badges_and_abstracts.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from openpyxl import Workbook, load_workbook\n",
    "\n",
    "# Base URL of the page to scrape\n",
    "base_url = \"https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&ConceptID=118290&ConceptID=118211&expand=all&startPage={}&pageSize=50\"\n",
    "\n",
    "# Excel file to store the titles, URLs, badges, and abstracts\n",
    "excel_file = \"titles_urls_badges_and_abstracts.xlsx\"\n",
    "\n",
    "# Clear the file at the start by creating a new workbook and writing the header\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Scraped Data\"\n",
    "ws.append([\"Title\", \"URL\", \"Badges\", \"Abstract\"])\n",
    "wb.save(excel_file)\n",
    "\n",
    "\n",
    "# Loop through the pages from startPage=0 to startPage=71\n",
    "for page in range(72):\n",
    "    print(f\"Getting page {page} of 40\")\n",
    "    # Construct the URL for the current page\n",
    "    url = base_url.format(page)\n",
    "\n",
    "    # Send a request to the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we got a valid response\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all the list items with class \"search__item issue-item-container\"\n",
    "    search_items = soup.find_all(\"li\", class_=\"search__item issue-item-container\")\n",
    "\n",
    "    # Iterate over each search item and extract the necessary information\n",
    "    for item in search_items:\n",
    "        title_tag = item.find(\"span\", class_=\"hlFld-Title\")\n",
    "        if title_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link_tag = title_tag.find(\"a\")\n",
    "            href = link_tag[\"href\"] if link_tag else None\n",
    "            full_url = f\"https://dl.acm.org{href}\" if href else None\n",
    "\n",
    "            # Extract the alt text of the image in the publisher badge\n",
    "            badges = []\n",
    "            publisher_badge_div = item.find(\"div\", class_=\"publisher-badge\")\n",
    "            img_tag = publisher_badge_div.find(\"img\") if publisher_badge_div else None\n",
    "            if img_tag and \"alt\" in img_tag.attrs:\n",
    "                badges.append(img_tag[\"alt\"])\n",
    "\n",
    "            # Extract the data titles from the 'a' tags in the img-badget divs\n",
    "            img_badget_divs = item.find_all(\"div\", class_=\"img-badget\")\n",
    "            for div in img_badget_divs:\n",
    "                a_tag = div.find(\"a\")\n",
    "                if a_tag and \"data-title\" in a_tag.attrs:\n",
    "                    badges.append(a_tag[\"data-title\"])\n",
    "\n",
    "            # Join all badges with a semicolon\n",
    "            badges_str = \"; \".join(badges)\n",
    "\n",
    "            # Extract the abstract from the search results page\n",
    "            abstract_div = item.find(\"div\", class_=\"issue-item__abstract\")\n",
    "            abstract_p = abstract_div.find(\"p\") if abstract_div else None\n",
    "            abstract = abstract_p.get_text(strip=True) if abstract_p else None\n",
    "\n",
    "\n",
    "            # Append the data to the Excel file immediately\n",
    "            wb = load_workbook(excel_file)\n",
    "            ws = wb.active\n",
    "            ws.append([title, full_url, badges_str, abstract])\n",
    "            wb.save(excel_file)\n",
    "\n",
    "            # Sleep for 10 seconds between each search result request\n",
    "            time.sleep(2)\n",
    "\n",
    "    # Sleep for 10 seconds between each page request\n",
    "    time.sleep(10)\n",
    "\n",
    "print(f\"Data saved to {excel_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
